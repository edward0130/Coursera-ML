{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting a decision stump\n",
    "\n",
    "The goal of this notebook is to implement your own boosting module.\n",
    "\n",
    "**Brace yourselves**! This is going to be a fun and challenging assignment.\n",
    "\n",
    "\n",
    "* Use SFrames to do some feature engineering.\n",
    "* Modify the decision trees to incorporate weights.\n",
    "* Implement Adaboost ensembling.\n",
    "* Use your implementation of Adaboost to train a boosted decision stump ensemble.\n",
    "* Evaluate the effect of boosting (adding more decision stumps) on performance of the model.\n",
    "* Explore the robustness of Adaboost to overfitting.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire up GraphLab Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the latest version of GraphLab Create **(1.8.3 or newer)**. Upgrade by\n",
    "```\n",
    "   pip install graphlab-create --upgrade\n",
    "```\n",
    "See [this page](https://dato.com/download/) for detailed instructions on upgrading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the same [LendingClub](https://www.lendingclub.com/) dataset as in the previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to edward30@163.com and will expire on September 27, 2017.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: C:\\Users\\Edward\\AppData\\Local\\Temp\\graphlab_server_1504076739.log.0\n"
     ]
    }
   ],
   "source": [
    "loans = graphlab.SFrame('lending-club-data.gl/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the target and the feature columns\n",
    "\n",
    "We will now repeat some of the feature processing steps that we saw in the previous assignment:\n",
    "\n",
    "First, we re-assign the target to have +1 as a safe (good) loan, and -1 as a risky (bad) loan.\n",
    "\n",
    "Next, we select four categorical features: \n",
    "1. grade of the loan \n",
    "2. the length of the loan term\n",
    "3. the home ownership status: own, mortgage, rent\n",
    "4. number of years of employment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "loans.remove_column('bad_loans')\n",
    "target = 'safe_loans'\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample dataset to make sure classes are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we did in the previous assignment, we will undersample the larger class (safe loans) in order to balance out our dataset. This means we are throwing away many data points. We use `seed=1` so everyone gets the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of safe loans                 : 0.502236174422\n",
      "Percentage of risky loans                : 0.497763825578\n",
      "Total number of loans in our new dataset : 46508\n"
     ]
    }
   ],
   "source": [
    "safe_loans_raw = loans[loans[target] == 1]\n",
    "risky_loans_raw = loans[loans[target] == -1]\n",
    "\n",
    "# Undersample the safe loans.\n",
    "percentage = len(risky_loans_raw)/float(len(safe_loans_raw))\n",
    "risky_loans = risky_loans_raw\n",
    "safe_loans = safe_loans_raw.sample(percentage, seed=1)\n",
    "loans_data = risky_loans_raw.append(safe_loans)\n",
    "\n",
    "print \"Percentage of safe loans                 :\", len(safe_loans) / float(len(loans_data))\n",
    "print \"Percentage of risky loans                :\", len(risky_loans) / float(len(loans_data))\n",
    "print \"Total number of loans in our new dataset :\", len(loans_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There are many approaches for dealing with imbalanced data, including some where we modify the learning algorithm. These approaches are beyond the scope of this course, but some of them are reviewed in this [paper](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=5128907&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F69%2F5173046%2F05128907.pdf%3Farnumber%3D5128907 ). For this assignment, we use the simplest possible approach, where we subsample the overly represented class to get a more balanced dataset. In general, and especially when the data is highly imbalanced, we recommend using more advanced methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform categorical data into binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will work with **binary decision trees**. Since all of our features are currently categorical features, we want to turn them into binary features using 1-hot encoding. \n",
    "\n",
    "We can do so with the following code block (see the first assignments for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans_data = risky_loans.append(safe_loans)\n",
    "for feature in features:\n",
    "    loans_data_one_hot_encoded = loans_data[feature].apply(lambda x: {x: 1})    \n",
    "    loans_data_unpacked = loans_data_one_hot_encoded.unpack(column_name_prefix=feature)\n",
    "    \n",
    "    # Change None's to 0's\n",
    "    for column in loans_data_unpacked.column_names():\n",
    "        loans_data_unpacked[column] = loans_data_unpacked[column].fillna(0)\n",
    "\n",
    "    loans_data.remove_column(feature)\n",
    "    loans_data.add_columns(loans_data_unpacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the feature columns look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grade.A',\n",
       " 'grade.B',\n",
       " 'grade.C',\n",
       " 'grade.D',\n",
       " 'grade.E',\n",
       " 'grade.F',\n",
       " 'grade.G',\n",
       " 'term. 36 months',\n",
       " 'term. 60 months',\n",
       " 'home_ownership.MORTGAGE',\n",
       " 'home_ownership.OTHER',\n",
       " 'home_ownership.OWN',\n",
       " 'home_ownership.RENT',\n",
       " 'emp_length.1 year',\n",
       " 'emp_length.10+ years',\n",
       " 'emp_length.2 years',\n",
       " 'emp_length.3 years',\n",
       " 'emp_length.4 years',\n",
       " 'emp_length.5 years',\n",
       " 'emp_length.6 years',\n",
       " 'emp_length.7 years',\n",
       " 'emp_length.8 years',\n",
       " 'emp_length.9 years',\n",
       " 'emp_length.< 1 year',\n",
       " 'emp_length.n/a']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = loans_data.column_names()\n",
    "features.remove('safe_loans')  # Remove the response variable\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "\n",
    "We split the data into training and test sets with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = loans_data.random_split(0.8, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify our decision tree code from Module 5 to support weighting of individual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted error definition\n",
    "\n",
    "Consider a model with $N$ data points with:\n",
    "* Predictions $\\hat{y}_1 ... \\hat{y}_n$ \n",
    "* Target $y_1 ... y_n$ \n",
    "* Data point weights $\\alpha_1 ... \\alpha_n$.\n",
    "\n",
    "Then the **weighted error** is defined by:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$\n",
    "where $1[y_i \\neq \\hat{y_i}]$ is an indicator function that is set to $1$ if $y_i \\neq \\hat{y_i}$.\n",
    "\n",
    "\n",
    "### Write a function to compute weight of mistakes\n",
    "\n",
    "Write a function that calculates the weight of mistakes for making the \"weighted-majority\" predictions for a dataset. The function accepts two inputs:\n",
    "* `labels_in_node`: Targets $y_1 ... y_n$ \n",
    "* `data_weights`: Data point weights $\\alpha_1 ... \\alpha_n$\n",
    "\n",
    "We are interested in computing the (total) weight of mistakes, i.e.\n",
    "$$\n",
    "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}].\n",
    "$$\n",
    "This quantity is analogous to the number of mistakes, except that each mistake now carries different weight. It is related to the weighted error in the following way:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$\n",
    "\n",
    "The function **intermediate_node_weighted_mistakes** should first compute two weights: \n",
    " * $\\mathrm{WM}_{-1}$: weight of mistakes when all predictions are $\\hat{y}_i = -1$ i.e $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{-1}$)\n",
    " * $\\mathrm{WM}_{+1}$: weight of mistakes when all predictions are $\\hat{y}_i = +1$ i.e $\\mbox{WM}(\\mathbf{\\alpha}, \\mathbf{+1}$)\n",
    " \n",
    " where $\\mathbf{-1}$ and $\\mathbf{+1}$ are vectors where all values are -1 and +1 respectively.\n",
    " \n",
    "After computing $\\mathrm{WM}_{-1}$ and $\\mathrm{WM}_{+1}$, the function **intermediate_node_weighted_mistakes** should return the lower of the two weights of mistakes, along with the class associated with that weight. We have provided a skeleton for you with `YOUR CODE HERE` to be filled in several places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
    "    # Sum the weights of all entries with label +1\n",
    "    total_weight_positive = sum(data_weights[labels_in_node == +1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all -1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    weighted_mistakes_all_negative = total_weight_positive\n",
    "    \n",
    "    # Sum the weights of all entries with label -1\n",
    "    ### YOUR CODE HERE\n",
    "    total_weight_negative = sum(data_weights[labels_in_node == -1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all +1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    weighted_mistakes_all_positive = total_weight_negative\n",
    "    \n",
    "    # Return the tuple (weight, class_label) representing the lower of the two weights\n",
    "    #    class_label should be an integer of value +1 or -1.\n",
    "    # If the two weights are identical, return (weighted_mistakes_all_positive,+1)\n",
    "    ### YOUR CODE HERE\n",
    "    if weighted_mistakes_all_negative < weighted_mistakes_all_positive:\n",
    "        return (weighted_mistakes_all_negative,-1)\n",
    "    else:\n",
    "        return (weighted_mistakes_all_positive,+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Test your **intermediate_node_weighted_mistakes** function, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_labels = graphlab.SArray([-1, -1, 1, 1, 1])\n",
    "example_data_weights = graphlab.SArray([1., 2., .5, 1., 1.])\n",
    "if intermediate_node_weighted_mistakes(example_labels, example_data_weights) == (2.5, -1):\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
    "$$\n",
    "\n",
    "**Quiz Question:** If we set the weights $\\mathbf{\\alpha} = 1$ for all data points, how is the weight of mistakes $\\mbox{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ related to the `classification error`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pick best feature to split on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We continue modifying our decision tree code from the earlier assignment to incorporate weighting of individual data points. The next step is to pick the best feature to split on.\n",
    "\n",
    "The **best_splitting_feature** function is similar to the one from the earlier assignment with two minor modifications:\n",
    "  1. The function **best_splitting_feature** should now accept an extra parameter `data_weights` to take account of weights of data points.\n",
    "  2. Instead of computing the number of mistakes in the left and right side of the split, we compute the weight of mistakes for both sides, add up the two weights, and divide it by the total weight of the data.\n",
    "  \n",
    "Complete the following function. Comments starting with `DIFFERENT HERE` mark the sections where the weighted version differs from the original implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def best_splitting_feature(data, features, target, data_weights):\n",
    "    \n",
    "    # These variables will keep track of the best feature and the corresponding error\n",
    "    best_feature = None\n",
    "    best_error = float('+inf') \n",
    "    num_points = float(len(data))\n",
    "\n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        left_split = data[data[feature] == 0]\n",
    "        right_split = data[data[feature] == 1]\n",
    "        \n",
    "        # Apply the same filtering to data_weights to create left_data_weights, right_data_weights\n",
    "        ## YOUR CODE HERE\n",
    "        left_data_weights = data_weights[data[feature] == 0]\n",
    "        right_data_weights = data_weights[data[feature] == 1]\n",
    "                    \n",
    "        # DIFFERENT HERE\n",
    "        # Calculate the weight of mistakes for left and right sides\n",
    "        ## YOUR CODE HERE\n",
    "        left_weighted_mistakes, left_class = intermediate_node_weighted_mistakes(left_split[target], left_data_weights)\n",
    "        right_weighted_mistakes, right_class = intermediate_node_weighted_mistakes(right_split[target], right_data_weights)\n",
    "        \n",
    "        # DIFFERENT HERE\n",
    "        # Compute weighted error by computing\n",
    "        #  ( [weight of mistakes (left)] + [weight of mistakes (right)] ) / [total weight of all data points]\n",
    "        ## YOUR CODE HERE\n",
    "        error = (left_weighted_mistakes + right_weighted_mistakes) / sum(data_weights)\n",
    "        \n",
    "        # If this is the best error we have found so far, store the feature and the error\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "    \n",
    "    # Return the best feature we found\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Now, we have another checkpoint to make sure you are on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = graphlab.SArray(len(train_data)* [1.5])\n",
    "if best_splitting_feature(train_data, features, target, example_data_weights) == 'term. 36 months':\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**. If you get an exception in the line of \"the logical filter has different size than the array\", try upgradting your GraphLab Create installation to 1.8.3 or newer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Very Optional**. Relationship between weighted error and weight of mistakes\n",
    "\n",
    "By definition, the weighted error is the weight of mistakes divided by the weight of all data points, so\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i} = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}.\n",
    "$$\n",
    "\n",
    "In the code above, we obtain $\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ from the two weights of mistakes from both sides, $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}})$ and $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})$. First, notice that the overall weight of mistakes $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ can be broken into two weights of mistakes over either side of the split:\n",
    "$$\n",
    "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})\n",
    "= \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\n",
    "= \\sum_{\\mathrm{left}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\n",
    " + \\sum_{\\mathrm{right}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\\\\\n",
    "= \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})\n",
    "$$\n",
    "We then divide through by the total weight of all data points to obtain $\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})\n",
    "= \\frac{\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the tree\n",
    "\n",
    "With the above functions implemented correctly, we are now ready to build our decision tree. Recall from the previous assignments that each node in the decision tree is represented as a dictionary which contains the following keys:\n",
    "\n",
    "    { \n",
    "       'is_leaf'            : True/False.\n",
    "       'prediction'         : Prediction at the leaf node.\n",
    "       'left'               : (dictionary corresponding to the left tree).\n",
    "       'right'              : (dictionary corresponding to the right tree).\n",
    "       'features_remaining' : List of features that are posible splits.\n",
    "    }\n",
    "    \n",
    "Let us start with a function that creates a leaf node given a set of target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_leaf(target_values, data_weights):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'is_leaf': True}\n",
    "    \n",
    "    # Computed weight of mistakes.\n",
    "    weighted_error, best_class = intermediate_node_weighted_mistakes(target_values, data_weights)\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    leaf['prediction'] = best_class ## YOUR CODE HERE\n",
    "    \n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a function that learns a weighted decision tree recursively and implements 3 stopping conditions:\n",
    "1. All data points in a node are from the same class.\n",
    "2. No more features to split on.\n",
    "3. Stop growing the tree when the tree depth reaches **max_depth**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    target_values = data[target]\n",
    "    print \"--------------------------------------------------------------------\"\n",
    "    print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "    \n",
    "    # Stopping condition 1. Error is 0.\n",
    "    if intermediate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
    "        print \"Stopping condition 1 reached.\"                \n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # Stopping condition 2. No more features.\n",
    "    if remaining_features == []:\n",
    "        print \"Stopping condition 2 reached.\"                \n",
    "        return create_leaf(target_values, data_weights)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth > max_depth:\n",
    "        print \"Reached maximum depth. Stopping for now.\"\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    splitting_feature = best_splitting_feature(data, features, target, data_weights)\n",
    "    remaining_features.remove(splitting_feature)\n",
    "        \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    \n",
    "    left_data_weights = data_weights[data[splitting_feature] == 0]\n",
    "    right_data_weights = data_weights[data[splitting_feature] == 1]\n",
    "    \n",
    "    print \"Split on feature %s. (%s, %s)\" % (\\\n",
    "              splitting_feature, len(left_split), len(right_split))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(data):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(left_split[target], data_weights)\n",
    "    if len(right_split) == len(data):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(right_split[target], data_weights)\n",
    "    \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = weighted_decision_tree_create(\n",
    "        left_split, remaining_features, target, left_data_weights, current_depth + 1, max_depth)\n",
    "    right_tree = weighted_decision_tree_create(\n",
    "        right_split, remaining_features, target, right_data_weights, current_depth + 1, max_depth)\n",
    "    \n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a recursive function to count the nodes in your tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following test code to check your implementation. Make sure you get **'Test passed'** before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Split on feature grade.A. (9122, 101)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (9122 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (101 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Split on feature grade.D. (23300, 4701)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (23300 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (4701 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = graphlab.SArray([1.0 for i in range(len(train_data))])\n",
    "small_data_decision_tree = weighted_decision_tree_create(train_data, features, target,\n",
    "                                        example_data_weights, max_depth=2)\n",
    "if count_nodes(small_data_decision_tree) == 7:\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'\n",
    "    print 'Number of nodes found:', count_nodes(small_data_decision_tree)\n",
    "    print 'Number of nodes that should be there: 7' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a quick look at what the trained tree is like. You should get something that looks like the following\n",
    "\n",
    "```\n",
    "{'is_leaf': False,\n",
    "    'left': {'is_leaf': False,\n",
    "        'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
    "        'prediction': None,\n",
    "        'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
    "        'splitting_feature': 'grade.A'\n",
    "     },\n",
    "    'prediction': None,\n",
    "    'right': {'is_leaf': False,\n",
    "        'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
    "        'prediction': None,\n",
    "        'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
    "        'splitting_feature': 'grade.D'\n",
    "     },\n",
    "     'splitting_feature': 'term. 36 months'\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': False,\n",
       " 'left': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade.A'},\n",
       " 'prediction': None,\n",
       " 'right': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade.D'},\n",
       " 'splitting_feature': 'term. 36 months'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with a weighted decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give you a function that classifies one data point. It can also return the probability if you want to play around with that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # If the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print \"At leaf, predicting %s\" % tree['prediction']\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # Split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print \"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value)\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the tree\n",
    "\n",
    "Now, we will write a function to evaluate a decision tree by computing the classification error of the tree on the given dataset.\n",
    "\n",
    "Again, recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
    "$$\n",
    "\n",
    "The function called **evaluate_classification_error** takes in as input:\n",
    "1. `tree` (as described above)\n",
    "2. `data` (an SFrame)\n",
    "\n",
    "The function does not change because of adding data point weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = data.apply(lambda x: classify(tree, x))\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error\n",
    "    return (prediction != data[target]).sum() / float(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3981042654028436"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Training a weighted decision tree\n",
    "\n",
    "To build intuition on how weighted data points affect the tree being built, consider the following:\n",
    "\n",
    "Suppose we only care about making good predictions for the **first 10 and last 10 items** in `train_data`, we assign weights:\n",
    "* 1 to the last 10 items \n",
    "* 1 to the first 10 items \n",
    "* and 0 to the rest. \n",
    "\n",
    "Let us fit a weighted decision tree with `max_depth = 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership.RENT. (20514, 16710)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (20514 data points).\n",
      "Split on feature grade.F. (19613, 901)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19613 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (901 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16710 data points).\n",
      "Split on feature grade.D. (13315, 3395)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (13315 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (3395 data points).\n",
      "Stopping condition 1 reached.\n"
     ]
    }
   ],
   "source": [
    "# Assign weights\n",
    "example_data_weights = graphlab.SArray([1.] * 10 + [0.]*(len(train_data) - 20) + [1.] * 10)\n",
    "\n",
    "# Train a weighted decision tree model.\n",
    "small_data_decision_tree_subset_20 = weighted_decision_tree_create(train_data, features, target,\n",
    "                         example_data_weights, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compute the classification error on the `subset_20`, i.e. the subset of data points whose weight is 1 (namely the first and last 10 data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_20 = train_data.head(10).append(train_data.tail(10))\n",
    "evaluate_classification_error(small_data_decision_tree_subset_20, subset_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us compare the classification error of the model `small_data_decision_tree_subset_20` on the entire test set `train_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48124865678057166"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree_subset_20, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model `small_data_decision_tree_subset_20` performs **a lot** better on `subset_20` than on `train_data`.\n",
    "\n",
    "So, what does this mean?\n",
    "* The points with higher weights are the ones that are more important during the training process of the weighted decision tree.\n",
    "* The points with zero weights are basically ignored during training.\n",
    "\n",
    "**Quiz Question**: Will you get the same model as `small_data_decision_tree_subset_20` if you trained a decision tree with only the 20 data points with non-zero weights from the set of points in `subset_20`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing your own Adaboost (on decision stumps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a weighted decision tree working, it takes only a bit of work to implement Adaboost. For the sake of simplicity, let us stick with **decision tree stumps** by training trees with **`max_depth=1`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lecture the procedure for Adaboost:\n",
    "\n",
    "1\\. Start with unweighted data with $\\alpha_j = 1$\n",
    "\n",
    "2\\. For t = 1,...T:\n",
    "  * Learn $f_t(x)$ with data weights $\\alpha_j$\n",
    "  * Compute coefficient $\\hat{w}_t$:\n",
    "     $$\\hat{w}_t = \\frac{1}{2}\\ln{\\left(\\frac{1- \\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}\\right)}$$\n",
    "  * Re-compute weights $\\alpha_j$:\n",
    "     $$\\alpha_j \\gets \\begin{cases}\n",
    "     \\alpha_j \\exp{(-\\hat{w}_t)} & \\text{ if }f_t(x_j) = y_j\\\\\n",
    "     \\alpha_j \\exp{(\\hat{w}_t)} & \\text{ if }f_t(x_j) \\neq y_j\n",
    "     \\end{cases}$$\n",
    "  * Normalize weights $\\alpha_j$:\n",
    "      $$\\alpha_j \\gets \\frac{\\alpha_j}{\\sum_{i=1}^{N}{\\alpha_i}} $$\n",
    "  \n",
    "Complete the skeleton for the following code to implement **adaboost_with_tree_stumps**. Fill in the places with `YOUR CODE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "def adaboost_with_tree_stumps(data, features, target, num_tree_stumps):\n",
    "    # start with unweighted data\n",
    "    alpha = graphlab.SArray([1.]*len(data))\n",
    "    weights = []\n",
    "    tree_stumps = []\n",
    "    target_values = data[target]\n",
    "    \n",
    "    for t in xrange(num_tree_stumps):\n",
    "        print '====================================================='\n",
    "        print 'Adaboost Iteration %d' % t\n",
    "        print '====================================================='        \n",
    "        # Learn a weighted decision tree stump. Use max_depth=1\n",
    "        tree_stump = weighted_decision_tree_create(data, features, target, data_weights=alpha, max_depth=1)\n",
    "        tree_stumps.append(tree_stump)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x))\n",
    "        \n",
    "        # Produce a Boolean array indicating whether\n",
    "        # each data point was correctly classified\n",
    "        is_correct = predictions == target_values\n",
    "        is_wrong   = predictions != target_values\n",
    "        \n",
    "        # Compute weighted error\n",
    "        # YOUR CODE HERE\n",
    "        weighted_error = sum(alpha[is_wrong])/float(sum(alpha))\n",
    "\n",
    "        # Compute model coefficient using weighted error\n",
    "        # YOUR CODE HERE\n",
    "        weight =  log((1-weighted_error)/weighted_error)/2.0\n",
    "        weights.append(weight)\n",
    "        \n",
    "        # Adjust weights on data point\n",
    "        adjustment = is_correct.apply(lambda is_correct : exp(-weight) if is_correct else exp(weight))\n",
    "        # Scale alpha by multiplying by adjustment \n",
    "        # Then normalize data points weights\n",
    "        ## YOUR CODE HERE \n",
    "        alpha = alpha*adjustment\n",
    "        alpha_sum = sum(alpha)\n",
    "        alpha = alpha.apply(lambda x:  x/float(alpha_sum))\n",
    "    return weights, tree_stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking your Adaboost code\n",
    "\n",
    "Train an ensemble of **two** tree stumps and see which features those stumps split on. We will run the algorithm with the following parameters:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "is_correct\n",
      "[1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, ... ]\n",
      "21529\n",
      "is_wrong\n",
      "[0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, ... ]\n",
      "15695\n",
      "weighted_error\n",
      "0.421636578551\n",
      "weight\n",
      "0.158029336593\n",
      "adjustment\n",
      "[0.8538247332926777, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, ... ]\n",
      "[0.8538247332926777, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, ... ]\n",
      "[2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, ... ]\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "is_correct\n",
      "[1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, ... ]\n",
      "21090\n",
      "is_wrong\n",
      "[0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, ... ]\n",
      "16134\n",
      "weighted_error\n",
      "0.412498248915\n",
      "weight\n",
      "0.176823632936\n",
      "adjustment\n",
      "[0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, ... ]\n",
      "[1.9460438346271766e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 3.801913330611601e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 3.801913330611601e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 3.801913330611601e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 3.801913330611601e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 3.801913330611601e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 3.801913330611601e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.771658215613781e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 3.801913330611601e-05, 2.6694092204962397e-05, 2.771658215613781e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 3.801913330611601e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 3.801913330611601e-05, 3.801913330611601e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 3.801913330611601e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, ... ]\n",
      "[1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.815101392687598e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.815101392687598e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, ... ]\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, target, num_tree_stumps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stump(tree):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print \"(leaf, label: %s)\" % tree['prediction']\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('.')\n",
    "    print '                       root'\n",
    "    print '         |---------------|----------------|'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '  [{0} == 0]{1}[{0} == 1]    '.format(split_name, ' '*(27-len(split_name)))\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '    (%s)                 (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the first stump looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term. 36 months == 0]            [term. 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the next stump looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade.A == 0]                    [grade.A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15802933659263743, 0.1768236329364191]\n"
     ]
    }
   ],
   "source": [
    "print stump_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your Adaboost is correctly implemented, the following things should be true:\n",
    "\n",
    "* `tree_stumps[0]` should split on **term. 36 months** with the prediction -1 on the left and +1 on the right.\n",
    "* `tree_stumps[1]` should split on **grade.A** with the prediction -1 on the left and +1 on the right.\n",
    "* Weights should be approximately `[0.158, 0.177]` \n",
    "\n",
    "**Reminders**\n",
    "- Stump weights ($\\mathbf{\\hat{w}}$) and data point weights ($\\mathbf{\\alpha}$) are two different concepts.\n",
    "- Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble.\n",
    "- Data point weights ($\\mathbf{\\alpha}$) tell you how important each data point is while training a decision stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a boosted ensemble of 10 stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train an ensemble of 10 decision tree stumps with Adaboost. We run the **adaboost_with_tree_stumps** function with the following parameters:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "is_correct\n",
      "[1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, ... ]\n",
      "21529\n",
      "is_wrong\n",
      "[0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, ... ]\n",
      "15695\n",
      "weighted_error\n",
      "0.421636578551\n",
      "weight\n",
      "0.158029336593\n",
      "adjustment\n",
      "[0.8538247332926777, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, ... ]\n",
      "[0.8538247332926777, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 0.8538247332926777, 0.8538247332926777, 1.171200553237213, 1.171200553237213, 1.171200553237213, 1.171200553237213, ... ]\n",
      "[2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, ... ]\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "is_correct\n",
      "[1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, ... ]\n",
      "21090\n",
      "is_wrong\n",
      "[0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, ... ]\n",
      "16134\n",
      "weighted_error\n",
      "0.412498248915\n",
      "weight\n",
      "0.176823632936\n",
      "adjustment\n",
      "[0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 1.193420594479305, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, 0.8379275543139967, ... ]\n",
      "[1.9460438346271766e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 3.801913330611601e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 3.801913330611601e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 3.801913330611601e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 3.801913330611601e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 3.801913330611601e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 3.801913330611601e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.771658215613781e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 3.801913330611601e-05, 2.6694092204962397e-05, 2.771658215613781e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 3.801913330611601e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 3.801913330611601e-05, 3.801913330611601e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 3.801913330611601e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 1.9460438346271766e-05, 1.9460438346271766e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, 2.6694092204962397e-05, ... ]\n",
      "[1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.815101392687598e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.815101392687598e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, ... ]\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "is_correct\n",
      "[0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, ... ]\n",
      "20141\n",
      "is_wrong\n",
      "[1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, ... ]\n",
      "17083\n",
      "weighted_error\n",
      "0.453574664309\n",
      "weight\n",
      "0.0931188897113\n",
      "adjustment\n",
      "[1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 0.9110851752034518, 0.9110851752034518, 0.9110851752034518, 1.0975922199334358, 0.9110851752034518, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 0.9110851752034518, 0.9110851752034518, 0.9110851752034518, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 0.9110851752034518, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 0.9110851752034518, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 0.9110851752034518, 1.0975922199334358, 0.9110851752034518, 0.9110851752034518, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 0.9110851752034518, 0.9110851752034518, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 0.9110851752034518, 1.0975922199334358, 0.9110851752034518, 1.0975922199334358, 0.9110851752034518, 1.0975922199334358, 0.9110851752034518, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 1.0975922199334358, 0.9110851752034518, 1.0975922199334358, ... ]\n",
      "[2.1694418088067658e-05, 2.1694418088067658e-05, 2.1694418088067658e-05, 2.975846620057398e-05, 2.975846620057398e-05, 2.975846620057398e-05, 2.975846620057398e-05, 2.975846620057398e-05, 2.975846620057398e-05, 2.975846620057398e-05, 2.1694418088067658e-05, 1.8008020051292587e-05, 2.4701794436717304e-05, 1.8008020051292587e-05, 2.975846620057398e-05, 2.4701794436717304e-05, 2.975846620057398e-05, 2.1694418088067658e-05, 2.975846620057398e-05, 4.238357629014429e-05, 2.975846620057398e-05, 2.1694418088067658e-05, 2.975846620057398e-05, 4.238357629014429e-05, 2.975846620057398e-05, 2.4701794436717304e-05, 2.4701794436717304e-05, 2.4701794436717304e-05, 2.975846620057398e-05, 2.975846620057398e-05, 2.975846620057398e-05, 2.975846620057398e-05, 1.8008020051292587e-05, 2.1694418088067658e-05, 2.975846620057398e-05, 2.975846620057398e-05, 2.975846620057398e-05, 4.238357629014429e-05, 2.1694418088067658e-05, 2.1694418088067658e-05, 4.238357629014429e-05, 2.1694418088067658e-05, 1.8008020051292587e-05, 2.1694418088067658e-05, 2.1694418088067658e-05, 2.975846620057398e-05, 2.975846620057398e-05, 2.975846620057398e-05, 4.238357629014429e-05, 2.975846620057398e-05, 2.975846620057398e-05, 4.238357629014429e-05, 2.975846620057398e-05, 2.1694418088067658e-05, 2.975846620057398e-05, 3.0898333869376876e-05, 2.975846620057398e-05, 2.1694418088067658e-05, 2.975846620057398e-05, 2.975846620057398e-05, 2.1694418088067658e-05, 2.4701794436717304e-05, 2.975846620057398e-05, 2.4701794436717304e-05, 2.4701794436717304e-05, 4.238357629014429e-05, 2.975846620057398e-05, 3.0898333869376876e-05, 2.1694418088067658e-05, 2.1694418088067658e-05, 2.975846620057398e-05, 2.975846620057398e-05, 2.1694418088067658e-05, 4.238357629014429e-05, 2.975846620057398e-05, 2.1694418088067658e-05, 2.4701794436717304e-05, 1.8008020051292587e-05, 2.975846620057398e-05, 2.975846620057398e-05, 4.238357629014429e-05, 4.238357629014429e-05, 2.4701794436717304e-05, 2.1694418088067658e-05, 1.8008020051292587e-05, 4.238357629014429e-05, 2.4701794436717304e-05, 2.975846620057398e-05, 2.4701794436717304e-05, 2.1694418088067658e-05, 2.975846620057398e-05, 2.1694418088067658e-05, 2.975846620057398e-05, 2.975846620057398e-05, 2.1694418088067658e-05, 2.1694418088067658e-05, 2.975846620057398e-05, 2.975846620057398e-05, 2.4701794436717304e-05, 2.975846620057398e-05, ... ]\n",
      "[2.1788543607076036e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 1.8086151404102345e-05, 2.4808968052177085e-05, 1.8086151404102345e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.4808968052177085e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 1.8086151404102345e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 4.2567465809538e-05, 2.1788543607076036e-05, 1.8086151404102345e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 3.103239239540615e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.4808968052177085e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 3.103239239540615e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.4808968052177085e-05, 1.8086151404102345e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 4.2567465809538e-05, 2.4808968052177085e-05, 2.1788543607076036e-05, 1.8086151404102345e-05, 4.2567465809538e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, ... ]\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership.MORTGAGE. (19846, 17378)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (19846 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (17378 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "is_correct\n",
      "[1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, ... ]\n",
      "19828\n",
      "is_wrong\n",
      "[0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, ... ]\n",
      "17396\n",
      "weighted_error\n",
      "0.463619975981\n",
      "weight\n",
      "0.0728888552584\n",
      "adjustment\n",
      "[0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 1.075610981766239, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 1.075610981766239, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 1.075610981766239, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 1.075610981766239, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 1.075610981766239, 0.9297041560117956, 1.075610981766239, 1.075610981766239, 1.075610981766239, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 1.075610981766239, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 1.075610981766239, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 1.075610981766239, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 1.075610981766239, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 1.075610981766239, 1.075610981766239, 1.075610981766239, 1.075610981766239, 1.075610981766239, 1.075610981766239, 1.075610981766239, 0.9297041560117956, 1.075610981766239, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 0.9297041560117956, 1.075610981766239, 0.9297041560117956, 1.075610981766239, 0.9297041560117956, ... ]\n",
      "[2.025689954494283e-05, 2.025689954494283e-05, 2.025689954494283e-05, 2.778660658190979e-05, 2.778660658190979e-05, 2.778660658190979e-05, 2.778660658190979e-05, 2.778660658190979e-05, 3.214740839035362e-05, 2.778660658190979e-05, 2.025689954494283e-05, 1.6814770126652522e-05, 2.3065000704472898e-05, 1.9453663068139364e-05, 2.778660658190979e-05, 2.3065000704472898e-05, 2.778660658190979e-05, 2.025689954494283e-05, 3.214740839035362e-05, 3.957514987401749e-05, 2.778660658190979e-05, 2.025689954494283e-05, 3.214740839035362e-05, 3.957514987401749e-05, 2.778660658190979e-05, 2.3065000704472898e-05, 2.3065000704472898e-05, 2.3065000704472898e-05, 2.778660658190979e-05, 3.214740839035362e-05, 2.778660658190979e-05, 3.214740839035362e-05, 1.9453663068139364e-05, 2.3435996780463564e-05, 2.778660658190979e-05, 2.778660658190979e-05, 2.778660658190979e-05, 3.957514987401749e-05, 2.025689954494283e-05, 2.025689954494283e-05, 3.957514987401749e-05, 2.025689954494283e-05, 1.6814770126652522e-05, 2.025689954494283e-05, 2.025689954494283e-05, 2.778660658190979e-05, 2.778660658190979e-05, 2.778660658190979e-05, 4.578603369069798e-05, 2.778660658190979e-05, 2.778660658190979e-05, 3.957514987401749e-05, 3.214740839035362e-05, 2.025689954494283e-05, 2.778660658190979e-05, 2.8850944180997936e-05, 2.778660658190979e-05, 2.025689954494283e-05, 2.778660658190979e-05, 2.778660658190979e-05, 2.025689954494283e-05, 2.3065000704472898e-05, 3.214740839035362e-05, 2.3065000704472898e-05, 2.3065000704472898e-05, 3.957514987401749e-05, 2.778660658190979e-05, 2.8850944180997936e-05, 2.025689954494283e-05, 2.025689954494283e-05, 2.778660658190979e-05, 2.778660658190979e-05, 2.025689954494283e-05, 4.578603369069798e-05, 2.778660658190979e-05, 2.025689954494283e-05, 2.3065000704472898e-05, 1.6814770126652522e-05, 2.778660658190979e-05, 2.778660658190979e-05, 4.578603369069798e-05, 4.578603369069798e-05, 2.668479848320945e-05, 2.3435996780463564e-05, 1.9453663068139364e-05, 4.578603369069798e-05, 2.668479848320945e-05, 2.778660658190979e-05, 2.668479848320945e-05, 2.025689954494283e-05, 2.778660658190979e-05, 2.025689954494283e-05, 2.778660658190979e-05, 2.778660658190979e-05, 2.025689954494283e-05, 2.025689954494283e-05, 3.214740839035362e-05, 2.778660658190979e-05, 2.668479848320945e-05, 2.778660658190979e-05, ... ]\n",
      "[2.0310733650963335e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 3.2232842342757394e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 1.685945653661868e-05, 2.3126297532772446e-05, 1.9505362517888515e-05, 2.7860451403095862e-05, 2.3126297532772446e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 3.2232842342757394e-05, 3.968032356110462e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 3.2232842342757394e-05, 3.968032356110462e-05, 2.7860451403095862e-05, 2.3126297532772446e-05, 2.3126297532772446e-05, 2.3126297532772446e-05, 2.7860451403095862e-05, 3.2232842342757394e-05, 2.7860451403095862e-05, 3.2232842342757394e-05, 1.9505362517888515e-05, 2.3498279556392647e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 3.968032356110462e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 3.968032356110462e-05, 2.0310733650963335e-05, 1.685945653661868e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 4.5907713229390207e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 3.968032356110462e-05, 3.2232842342757394e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.8927617552674857e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.3126297532772446e-05, 3.2232842342757394e-05, 2.3126297532772446e-05, 2.3126297532772446e-05, 3.968032356110462e-05, 2.7860451403095862e-05, 2.8927617552674857e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 4.5907713229390207e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.3126297532772446e-05, 1.685945653661868e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 4.5907713229390207e-05, 4.5907713229390207e-05, 2.6755715173470646e-05, 2.3498279556392647e-05, 1.9505362517888515e-05, 4.5907713229390207e-05, 2.6755715173470646e-05, 2.7860451403095862e-05, 2.6755715173470646e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 3.2232842342757394e-05, 2.7860451403095862e-05, 2.6755715173470646e-05, 2.7860451403095862e-05, ... ]\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "is_correct\n",
      "[1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, ... ]\n",
      "20156\n",
      "is_wrong\n",
      "[0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, ... ]\n",
      "17068\n",
      "weighted_error\n",
      "0.46651864402\n",
      "weight\n",
      "0.0670630691412\n",
      "adjustment\n",
      "[0.9351362212385229, 0.9351362212385229, 1.0693629198487997, 0.9351362212385229, 1.0693629198487997, 1.0693629198487997, 1.0693629198487997, 0.9351362212385229, 1.0693629198487997, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 1.0693629198487997, 0.9351362212385229, 1.0693629198487997, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 1.0693629198487997, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 1.0693629198487997, 1.0693629198487997, 0.9351362212385229, 1.0693629198487997, 0.9351362212385229, 1.0693629198487997, 1.0693629198487997, 1.0693629198487997, 1.0693629198487997, 0.9351362212385229, 0.9351362212385229, 1.0693629198487997, 0.9351362212385229, 1.0693629198487997, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 1.0693629198487997, 1.0693629198487997, 1.0693629198487997, 0.9351362212385229, 1.0693629198487997, 1.0693629198487997, 0.9351362212385229, 1.0693629198487997, 1.0693629198487997, 1.0693629198487997, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 1.0693629198487997, 0.9351362212385229, 1.0693629198487997, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 1.0693629198487997, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 1.0693629198487997, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 1.0693629198487997, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 1.0693629198487997, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 1.0693629198487997, 0.9351362212385229, 1.0693629198487997, 0.9351362212385229, 0.9351362212385229, 0.9351362212385229, 1.0693629198487997, ... ]\n",
      "[1.8993302716943962e-05, 1.8993302716943962e-05, 2.1719545441265423e-05, 2.605331724709057e-05, 2.979293366072018e-05, 2.979293366072018e-05, 2.979293366072018e-05, 2.605331724709057e-05, 3.446860640267707e-05, 2.605331724709057e-05, 1.8993302716943962e-05, 1.5765888477788705e-05, 2.16262384860346e-05, 1.8240170998865785e-05, 2.979293366072018e-05, 2.16262384860346e-05, 2.979293366072018e-05, 1.8993302716943962e-05, 3.0142098388183208e-05, 3.71065078324533e-05, 2.979293366072018e-05, 1.8993302716943962e-05, 3.0142098388183208e-05, 3.71065078324533e-05, 2.605331724709057e-05, 2.16262384860346e-05, 2.16262384860346e-05, 2.16262384860346e-05, 2.979293366072018e-05, 3.446860640267707e-05, 2.605331724709057e-05, 3.446860640267707e-05, 1.8240170998865785e-05, 2.51281888378474e-05, 2.979293366072018e-05, 2.979293366072018e-05, 2.979293366072018e-05, 3.71065078324533e-05, 1.8993302716943962e-05, 2.1719545441265423e-05, 3.71065078324533e-05, 2.1719545441265423e-05, 1.5765888477788705e-05, 1.8993302716943962e-05, 1.8993302716943962e-05, 2.979293366072018e-05, 2.979293366072018e-05, 2.979293366072018e-05, 4.2929965475033703e-05, 2.979293366072018e-05, 2.979293366072018e-05, 3.71065078324533e-05, 3.446860640267707e-05, 2.1719545441265423e-05, 2.979293366072018e-05, 2.7051262967641534e-05, 2.605331724709057e-05, 1.8993302716943962e-05, 2.605331724709057e-05, 2.605331724709057e-05, 2.1719545441265423e-05, 2.16262384860346e-05, 3.446860640267707e-05, 2.16262384860346e-05, 2.16262384860346e-05, 3.71065078324533e-05, 2.605331724709057e-05, 2.7051262967641534e-05, 1.8993302716943962e-05, 1.8993302716943962e-05, 2.605331724709057e-05, 2.979293366072018e-05, 1.8993302716943962e-05, 4.2929965475033703e-05, 2.605331724709057e-05, 1.8993302716943962e-05, 2.16262384860346e-05, 1.5765888477788705e-05, 2.979293366072018e-05, 2.605331724709057e-05, 4.2929965475033703e-05, 4.2929965475033703e-05, 2.502023838385355e-05, 2.51281888378474e-05, 1.8240170998865785e-05, 4.2929965475033703e-05, 2.502023838385355e-05, 2.979293366072018e-05, 2.502023838385355e-05, 1.8993302716943962e-05, 2.605331724709057e-05, 1.8993302716943962e-05, 2.605331724709057e-05, 2.979293366072018e-05, 1.8993302716943962e-05, 2.1719545441265423e-05, 3.0142098388183208e-05, 2.605331724709057e-05, 2.502023838385355e-05, 2.979293366072018e-05, ... ]\n",
      "[1.9036029491260022e-05, 1.9036029491260022e-05, 2.1768405090908584e-05, 2.611192602213042e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.611192602213042e-05, 3.454614596431647e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 1.580135495610065e-05, 2.1674888235099766e-05, 1.8281203550253472e-05, 2.9859954966688172e-05, 2.1674888235099766e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 3.0209905164836932e-05, 3.718998153944582e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 3.0209905164836932e-05, 3.718998153944582e-05, 2.611192602213042e-05, 2.1674888235099766e-05, 2.1674888235099766e-05, 2.1674888235099766e-05, 2.9859954966688172e-05, 3.454614596431647e-05, 2.611192602213042e-05, 3.454614596431647e-05, 1.8281203550253472e-05, 2.518471647126885e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 3.718998153944582e-05, 1.9036029491260022e-05, 2.1768405090908584e-05, 3.718998153944582e-05, 2.1768405090908584e-05, 1.580135495610065e-05, 1.9036029491260022e-05, 1.9036029491260022e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 4.302653946080036e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 3.718998153944582e-05, 3.454614596431647e-05, 2.1768405090908584e-05, 2.9859954966688172e-05, 2.711211669197836e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 2.611192602213042e-05, 2.1768405090908584e-05, 2.1674888235099766e-05, 3.454614596431647e-05, 2.1674888235099766e-05, 2.1674888235099766e-05, 3.718998153944582e-05, 2.611192602213042e-05, 2.711211669197836e-05, 1.9036029491260022e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 4.302653946080036e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 2.1674888235099766e-05, 1.580135495610065e-05, 2.9859954966688172e-05, 2.611192602213042e-05, 4.302653946080036e-05, 4.302653946080036e-05, 2.5076523175113537e-05, 2.518471647126885e-05, 1.8281203550253472e-05, 4.302653946080036e-05, 2.5076523175113537e-05, 2.9859954966688172e-05, 2.5076523175113537e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 2.1768405090908584e-05, 3.0209905164836932e-05, 2.611192602213042e-05, 2.5076523175113537e-05, 2.9859954966688172e-05, ... ]\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "is_correct\n",
      "[0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, ... ]\n",
      "19955\n",
      "is_wrong\n",
      "[1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, ... ]\n",
      "17269\n",
      "weighted_error\n",
      "0.467760207218\n",
      "weight\n",
      "0.0645691696164\n",
      "adjustment\n",
      "[1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 0.9374712674913623, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 0.9374712674913623, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 0.9374712674913623, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 0.9374712674913623, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 0.9374712674913623, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 0.9374712674913623, 1.0666993588784455, 1.0666993588784455, 0.9374712674913623, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 0.9374712674913623, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, 1.0666993588784455, ... ]\n",
      "[2.0305720453918247e-05, 2.0305720453918247e-05, 2.3220343754278475e-05, 2.7853574746887916e-05, 3.185159481910553e-05, 3.185159481910553e-05, 3.185159481910553e-05, 2.7853574746887916e-05, 3.6850351751857576e-05, 2.7853574746887916e-05, 1.7845730695174485e-05, 1.6855295201083308e-05, 2.3120589384142882e-05, 1.950054810658174e-05, 3.185159481910553e-05, 2.3120589384142882e-05, 3.185159481910553e-05, 1.7845730695174485e-05, 3.2224886471110197e-05, 3.967052946482808e-05, 3.185159481910553e-05, 2.0305720453918247e-05, 3.2224886471110197e-05, 3.967052946482808e-05, 2.7853574746887916e-05, 2.3120589384142882e-05, 2.3120589384142882e-05, 2.3120589384142882e-05, 3.185159481910553e-05, 3.6850351751857576e-05, 2.7853574746887916e-05, 3.6850351751857576e-05, 1.950054810658174e-05, 2.686452091343791e-05, 3.185159481910553e-05, 3.185159481910553e-05, 3.185159481910553e-05, 3.967052946482808e-05, 2.0305720453918247e-05, 2.3220343754278475e-05, 3.967052946482808e-05, 2.3220343754278475e-05, 1.6855295201083308e-05, 1.7845730695174485e-05, 2.0305720453918247e-05, 3.185159481910553e-05, 3.185159481910553e-05, 3.185159481910553e-05, 4.5896382057593884e-05, 3.185159481910553e-05, 3.185159481910553e-05, 3.967052946482808e-05, 3.6850351751857576e-05, 2.3220343754278475e-05, 3.185159481910553e-05, 2.8920477493170916e-05, 2.4479180384607293e-05, 2.0305720453918247e-05, 2.7853574746887916e-05, 2.7853574746887916e-05, 2.3220343754278475e-05, 2.3120589384142882e-05, 3.6850351751857576e-05, 2.3120589384142882e-05, 2.3120589384142882e-05, 3.967052946482808e-05, 2.7853574746887916e-05, 2.8920477493170916e-05, 1.7845730695174485e-05, 2.0305720453918247e-05, 2.7853574746887916e-05, 3.185159481910553e-05, 1.7845730695174485e-05, 4.5896382057593884e-05, 2.7853574746887916e-05, 1.7845730695174485e-05, 2.3120589384142882e-05, 1.6855295201083308e-05, 3.185159481910553e-05, 2.7853574746887916e-05, 4.5896382057593884e-05, 4.5896382057593884e-05, 2.674911119379409e-05, 2.686452091343791e-05, 1.950054810658174e-05, 4.5896382057593884e-05, 2.674911119379409e-05, 3.185159481910553e-05, 2.674911119379409e-05, 2.0305720453918247e-05, 2.4479180384607293e-05, 2.0305720453918247e-05, 2.7853574746887916e-05, 3.185159481910553e-05, 2.0305720453918247e-05, 2.3220343754278475e-05, 3.2224886471110197e-05, 2.7853574746887916e-05, 2.674911119379409e-05, 3.185159481910553e-05, ... ]\n",
      "[2.0348064240517002e-05, 2.0348064240517002e-05, 2.3268765443275537e-05, 2.791165817356422e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 2.791165817356422e-05, 3.692719627624719e-05, 2.791165817356422e-05, 1.7882944632693728e-05, 1.6890443770407573e-05, 2.3168803054036615e-05, 1.954121285666819e-05, 3.1918015369753364e-05, 2.3168803054036615e-05, 3.1918015369753364e-05, 1.7882944632693728e-05, 3.2292085451761906e-05, 3.9753254942987325e-05, 3.1918015369753364e-05, 2.0348064240517002e-05, 3.2292085451761906e-05, 3.9753254942987325e-05, 2.791165817356422e-05, 2.3168803054036615e-05, 2.3168803054036615e-05, 2.3168803054036615e-05, 3.1918015369753364e-05, 3.692719627624719e-05, 2.791165817356422e-05, 3.692719627624719e-05, 1.954121285666819e-05, 2.692054185311439e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.9753254942987325e-05, 2.0348064240517002e-05, 2.3268765443275537e-05, 3.9753254942987325e-05, 2.3268765443275537e-05, 1.6890443770407573e-05, 1.7882944632693728e-05, 2.0348064240517002e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 4.5992090388254305e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.9753254942987325e-05, 3.692719627624719e-05, 2.3268765443275537e-05, 3.1918015369753364e-05, 2.8980785746211437e-05, 2.453022714222767e-05, 2.0348064240517002e-05, 2.791165817356422e-05, 2.791165817356422e-05, 2.3268765443275537e-05, 2.3168803054036615e-05, 3.692719627624719e-05, 2.3168803054036615e-05, 2.3168803054036615e-05, 3.9753254942987325e-05, 2.791165817356422e-05, 2.8980785746211437e-05, 1.7882944632693728e-05, 2.0348064240517002e-05, 2.791165817356422e-05, 3.1918015369753364e-05, 1.7882944632693728e-05, 4.5992090388254305e-05, 2.791165817356422e-05, 1.7882944632693728e-05, 2.3168803054036615e-05, 1.6890443770407573e-05, 3.1918015369753364e-05, 2.791165817356422e-05, 4.5992090388254305e-05, 4.5992090388254305e-05, 2.680489146806049e-05, 2.692054185311439e-05, 1.954121285666819e-05, 4.5992090388254305e-05, 2.680489146806049e-05, 3.1918015369753364e-05, 2.680489146806049e-05, 2.0348064240517002e-05, 2.453022714222767e-05, 2.0348064240517002e-05, 2.791165817356422e-05, 3.1918015369753364e-05, 2.0348064240517002e-05, 2.3268765443275537e-05, 3.2292085451761906e-05, 2.791165817356422e-05, 2.680489146806049e-05, 3.1918015369753364e-05, ... ]\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "is_correct\n",
      "[1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, ... ]\n",
      "21090\n",
      "is_wrong\n",
      "[0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, ... ]\n",
      "16134\n",
      "weighted_error\n",
      "0.472746758716\n",
      "weight\n",
      "0.0545605577918\n",
      "adjustment\n",
      "[0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 1.0560764281337625, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 1.0560764281337625, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 1.0560764281337625, 0.9469011648779458, 0.9469011648779458, 1.0560764281337625, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 1.0560764281337625, 0.9469011648779458, 0.9469011648779458, 1.0560764281337625, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 1.0560764281337625, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 1.0560764281337625, 0.9469011648779458, 1.0560764281337625, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 1.0560764281337625, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 1.0560764281337625, 1.0560764281337625, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 1.0560764281337625, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, 0.9469011648779458, ... ]\n",
      "[1.9267605732356825e-05, 1.9267605732356825e-05, 2.2033221103509298e-05, 2.6429581638222998e-05, 3.022320593421164e-05, 3.022320593421164e-05, 3.022320593421164e-05, 2.6429581638222998e-05, 3.4966405169655005e-05, 2.6429581638222998e-05, 1.69333811041455e-05, 1.5993580881504373e-05, 2.193856660069498e-05, 1.8503597217107e-05, 3.022320593421164e-05, 2.193856660069498e-05, 3.022320593421164e-05, 1.69333811041455e-05, 3.057741333061152e-05, 4.198247548688089e-05, 3.022320593421164e-05, 1.9267605732356825e-05, 3.057741333061152e-05, 4.198247548688089e-05, 2.6429581638222998e-05, 2.193856660069498e-05, 2.193856660069498e-05, 2.193856660069498e-05, 3.022320593421164e-05, 3.4966405169655005e-05, 2.6429581638222998e-05, 3.4966405169655005e-05, 1.8503597217107e-05, 2.549109243985951e-05, 3.022320593421164e-05, 3.022320593421164e-05, 3.022320593421164e-05, 4.198247548688089e-05, 1.9267605732356825e-05, 2.2033221103509298e-05, 4.198247548688089e-05, 2.2033221103509298e-05, 1.5993580881504373e-05, 1.69333811041455e-05, 1.9267605732356825e-05, 3.022320593421164e-05, 3.022320593421164e-05, 3.022320593421164e-05, 4.8571162539632754e-05, 3.022320593421164e-05, 3.022320593421164e-05, 4.198247548688089e-05, 3.4966405169655005e-05, 2.2033221103509298e-05, 3.022320593421164e-05, 3.060592469536883e-05, 2.3227700655695985e-05, 1.9267605732356825e-05, 2.6429581638222998e-05, 2.6429581638222998e-05, 2.2033221103509298e-05, 2.193856660069498e-05, 3.4966405169655005e-05, 2.193856660069498e-05, 2.193856660069498e-05, 4.198247548688089e-05, 2.6429581638222998e-05, 3.060592469536883e-05, 1.69333811041455e-05, 1.9267605732356825e-05, 2.6429581638222998e-05, 3.022320593421164e-05, 1.69333811041455e-05, 4.8571162539632754e-05, 2.6429581638222998e-05, 1.69333811041455e-05, 2.193856660069498e-05, 1.5993580881504373e-05, 3.022320593421164e-05, 2.6429581638222998e-05, 4.8571162539632754e-05, 4.8571162539632754e-05, 2.538158295553339e-05, 2.549109243985951e-05, 1.8503597217107e-05, 4.8571162539632754e-05, 2.538158295553339e-05, 3.022320593421164e-05, 2.538158295553339e-05, 1.9267605732356825e-05, 2.3227700655695985e-05, 1.9267605732356825e-05, 2.6429581638222998e-05, 3.022320593421164e-05, 1.9267605732356825e-05, 2.2033221103509298e-05, 3.057741333061152e-05, 2.6429581638222998e-05, 2.538158295553339e-05, 3.022320593421164e-05, ... ]\n",
      "[1.929629127644232e-05, 1.929629127644232e-05, 2.2066024086095184e-05, 2.6468929907010296e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 2.6468929907010296e-05, 3.501846303148477e-05, 2.6468929907010296e-05, 1.6958591462761244e-05, 1.601739206883269e-05, 2.1971228661987836e-05, 1.853114530798475e-05, 3.0268202137594344e-05, 2.1971228661987836e-05, 3.0268202137594344e-05, 1.6958591462761244e-05, 3.062293687672824e-05, 4.2044978849681946e-05, 3.0268202137594344e-05, 1.929629127644232e-05, 3.062293687672824e-05, 4.2044978849681946e-05, 2.6468929907010296e-05, 2.1971228661987836e-05, 2.1971228661987836e-05, 2.1971228661987836e-05, 3.0268202137594344e-05, 3.501846303148477e-05, 2.6468929907010296e-05, 3.501846303148477e-05, 1.853114530798475e-05, 2.5529043489207745e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 4.2044978849681946e-05, 1.929629127644232e-05, 2.2066024086095184e-05, 4.2044978849681946e-05, 2.2066024086095184e-05, 1.601739206883269e-05, 1.6958591462761244e-05, 1.929629127644232e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 4.8643475116694395e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 4.2044978849681946e-05, 3.501846303148477e-05, 2.2066024086095184e-05, 3.0268202137594344e-05, 3.0651490689105764e-05, 2.3262281975265167e-05, 1.929629127644232e-05, 2.6468929907010296e-05, 2.6468929907010296e-05, 2.2066024086095184e-05, 2.1971228661987836e-05, 3.501846303148477e-05, 2.1971228661987836e-05, 2.1971228661987836e-05, 4.2044978849681946e-05, 2.6468929907010296e-05, 3.0651490689105764e-05, 1.6958591462761244e-05, 1.929629127644232e-05, 2.6468929907010296e-05, 3.0268202137594344e-05, 1.6958591462761244e-05, 4.8643475116694395e-05, 2.6468929907010296e-05, 1.6958591462761244e-05, 2.1971228661987836e-05, 1.601739206883269e-05, 3.0268202137594344e-05, 2.6468929907010296e-05, 4.8643475116694395e-05, 4.8643475116694395e-05, 2.5419370967544032e-05, 2.5529043489207745e-05, 1.853114530798475e-05, 4.8643475116694395e-05, 2.5419370967544032e-05, 3.0268202137594344e-05, 2.5419370967544032e-05, 1.929629127644232e-05, 2.3262281975265167e-05, 1.929629127644232e-05, 2.6468929907010296e-05, 3.0268202137594344e-05, 1.929629127644232e-05, 2.2066024086095184e-05, 3.062293687672824e-05, 2.6468929907010296e-05, 2.5419370967544032e-05, 3.0268202137594344e-05, ... ]\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "is_correct\n",
      "[0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, ... ]\n",
      "19536\n",
      "is_wrong\n",
      "[1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, ... ]\n",
      "17688\n",
      "weighted_error\n",
      "0.478258250407\n",
      "weight\n",
      "0.0435109367336\n",
      "adjustment\n",
      "[1.0444714173552325, 0.9574220829634178, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 0.9574220829634178, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, 1.0444714173552325, ... ]\n",
      "[2.015442469920512e-05, 1.8474695387360233e-05, 2.3047331452598537e-05, 2.7646040735851347e-05, 3.161427198744784e-05, 3.161427198744784e-05, 3.161427198744784e-05, 2.7646040735851347e-05, 3.6575783716096714e-05, 2.7646040735851347e-05, 1.7712764061458582e-05, 1.6729708196468142e-05, 2.2948320341622343e-05, 1.93552516050466e-05, 3.161427198744784e-05, 2.2948320341622343e-05, 3.161427198744784e-05, 1.7712764061458582e-05, 3.198478228321616e-05, 4.3914778651798075e-05, 3.161427198744784e-05, 2.015442469920512e-05, 3.198478228321616e-05, 4.3914778651798075e-05, 2.7646040735851347e-05, 2.2948320341622343e-05, 2.2948320341622343e-05, 2.2948320341622343e-05, 3.161427198744784e-05, 3.6575783716096714e-05, 2.7646040735851347e-05, 3.6575783716096714e-05, 1.93552516050466e-05, 2.6664356236896184e-05, 3.161427198744784e-05, 3.161427198744784e-05, 3.161427198744784e-05, 4.3914778651798075e-05, 2.015442469920512e-05, 2.3047331452598537e-05, 4.3914778651798075e-05, 2.3047331452598537e-05, 1.6729708196468142e-05, 1.7712764061458582e-05, 2.015442469920512e-05, 3.161427198744784e-05, 3.161427198744784e-05, 3.161427198744784e-05, 5.0806719400217777e-05, 3.161427198744784e-05, 3.161427198744784e-05, 4.3914778651798075e-05, 3.6575783716096714e-05, 2.3047331452598537e-05, 3.161427198744784e-05, 3.201460592410101e-05, 2.4296788625622287e-05, 2.015442469920512e-05, 2.7646040735851347e-05, 2.7646040735851347e-05, 2.3047331452598537e-05, 2.2948320341622343e-05, 3.6575783716096714e-05, 2.2948320341622343e-05, 2.2948320341622343e-05, 4.3914778651798075e-05, 2.7646040735851347e-05, 3.201460592410101e-05, 1.7712764061458582e-05, 2.015442469920512e-05, 2.7646040735851347e-05, 3.161427198744784e-05, 1.7712764061458582e-05, 5.0806719400217777e-05, 2.7646040735851347e-05, 1.7712764061458582e-05, 2.2948320341622343e-05, 1.6729708196468142e-05, 3.161427198744784e-05, 2.7646040735851347e-05, 5.0806719400217777e-05, 5.0806719400217777e-05, 2.6549806422749164e-05, 2.6664356236896184e-05, 1.93552516050466e-05, 5.0806719400217777e-05, 2.6549806422749164e-05, 3.161427198744784e-05, 2.6549806422749164e-05, 1.8474695387360233e-05, 2.4296788625622287e-05, 2.015442469920512e-05, 2.7646040735851347e-05, 3.161427198744784e-05, 2.015442469920512e-05, 2.3047331452598537e-05, 3.198478228321616e-05, 2.7646040735851347e-05, 2.6549806422749164e-05, 3.161427198744784e-05, ... ]\n",
      "[2.0173505904006534e-05, 1.8492186308167507e-05, 2.3069151517330548e-05, 2.7672214629331413e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 2.7672214629331413e-05, 3.6610411845168866e-05, 2.7672214629331413e-05, 1.7729533623661425e-05, 1.674554705037387e-05, 2.2970046667569232e-05, 1.937357619259354e-05, 3.1644202804498845e-05, 2.2970046667569232e-05, 3.1644202804498845e-05, 1.7729533623661425e-05, 3.201506388094883e-05, 4.3956354975497195e-05, 3.1644202804498845e-05, 2.0173505904006534e-05, 3.201506388094883e-05, 4.3956354975497195e-05, 2.7672214629331413e-05, 2.2970046667569232e-05, 2.2970046667569232e-05, 2.2970046667569232e-05, 3.1644202804498845e-05, 3.6610411845168866e-05, 2.7672214629331413e-05, 3.6610411845168866e-05, 1.937357619259354e-05, 2.668960072042015e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 4.3956354975497195e-05, 2.0173505904006534e-05, 2.3069151517330548e-05, 4.3956354975497195e-05, 2.3069151517330548e-05, 1.674554705037387e-05, 1.7729533623661425e-05, 2.0173505904006534e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 5.085482066992069e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 4.3956354975497195e-05, 3.6610411845168866e-05, 2.3069151517330548e-05, 3.1644202804498845e-05, 3.204491575737045e-05, 2.431979161413232e-05, 2.0173505904006534e-05, 2.7672214629331413e-05, 2.7672214629331413e-05, 2.3069151517330548e-05, 2.2970046667569232e-05, 3.6610411845168866e-05, 2.2970046667569232e-05, 2.2970046667569232e-05, 4.3956354975497195e-05, 2.7672214629331413e-05, 3.204491575737045e-05, 1.7729533623661425e-05, 2.0173505904006534e-05, 2.7672214629331413e-05, 3.1644202804498845e-05, 1.7729533623661425e-05, 5.085482066992069e-05, 2.7672214629331413e-05, 1.7729533623661425e-05, 2.2970046667569232e-05, 1.674554705037387e-05, 3.1644202804498845e-05, 2.7672214629331413e-05, 5.085482066992069e-05, 5.085482066992069e-05, 2.65749424562183e-05, 2.668960072042015e-05, 1.937357619259354e-05, 5.085482066992069e-05, 2.65749424562183e-05, 3.1644202804498845e-05, 2.65749424562183e-05, 1.8492186308167507e-05, 2.431979161413232e-05, 2.0173505904006534e-05, 2.7672214629331413e-05, 3.1644202804498845e-05, 2.0173505904006534e-05, 2.3069151517330548e-05, 3.201506388094883e-05, 2.7672214629331413e-05, 2.65749424562183e-05, 3.1644202804498845e-05, ... ]\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "is_correct\n",
      "[1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, ... ]\n",
      "21090\n",
      "is_wrong\n",
      "[0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, ... ]\n",
      "16134\n",
      "weighted_error\n",
      "0.485509702974\n",
      "weight\n",
      "0.0289887115004\n",
      "adjustment\n",
      "[0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 1.0294129738816102, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 1.0294129738816102, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 1.0294129738816102, 0.9714274303628574, 0.9714274303628574, 1.0294129738816102, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 1.0294129738816102, 0.9714274303628574, 0.9714274303628574, 1.0294129738816102, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 1.0294129738816102, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 1.0294129738816102, 0.9714274303628574, 1.0294129738816102, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 1.0294129738816102, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 1.0294129738816102, 1.0294129738816102, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 1.0294129738816102, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, 0.9714274303628574, ... ]\n",
      "[1.9597097001739e-05, 1.7963817027134377e-05, 2.2410006579131825e-05, 2.6881548349820885e-05, 3.074004661625544e-05, 3.074004661625544e-05, 3.074004661625544e-05, 2.6881548349820885e-05, 3.556435830327831e-05, 2.6881548349820885e-05, 1.72229552895653e-05, 1.6267083741165012e-05, 2.2313733409591696e-05, 1.882002333771017e-05, 3.074004661625544e-05, 2.2313733409591696e-05, 3.074004661625544e-05, 1.72229552895653e-05, 3.110031123877285e-05, 4.5249242096322285e-05, 3.074004661625544e-05, 1.9597097001739e-05, 3.110031123877285e-05, 4.5249242096322285e-05, 2.6881548349820885e-05, 2.2313733409591696e-05, 2.2313733409591696e-05, 2.2313733409591696e-05, 3.074004661625544e-05, 3.556435830327831e-05, 2.6881548349820885e-05, 3.556435830327831e-05, 1.882002333771017e-05, 2.5927010245248413e-05, 3.074004661625544e-05, 3.074004661625544e-05, 3.074004661625544e-05, 4.5249242096322285e-05, 1.9597097001739e-05, 2.2410006579131825e-05, 4.5249242096322285e-05, 2.2410006579131825e-05, 1.6267083741165012e-05, 1.72229552895653e-05, 1.9597097001739e-05, 3.074004661625544e-05, 3.074004661625544e-05, 3.074004661625544e-05, 5.235061218203904e-05, 3.074004661625544e-05, 3.074004661625544e-05, 4.5249242096322285e-05, 3.556435830327831e-05, 2.2410006579131825e-05, 3.074004661625544e-05, 3.2987452027580386e-05, 2.362491267467673e-05, 1.9597097001739e-05, 2.6881548349820885e-05, 2.6881548349820885e-05, 2.2410006579131825e-05, 2.2313733409591696e-05, 3.556435830327831e-05, 2.2313733409591696e-05, 2.2313733409591696e-05, 4.5249242096322285e-05, 2.6881548349820885e-05, 3.2987452027580386e-05, 1.72229552895653e-05, 1.9597097001739e-05, 2.6881548349820885e-05, 3.074004661625544e-05, 1.72229552895653e-05, 5.235061218203904e-05, 2.6881548349820885e-05, 1.72229552895653e-05, 2.2313733409591696e-05, 1.6267083741165012e-05, 3.074004661625544e-05, 2.6881548349820885e-05, 5.235061218203904e-05, 5.235061218203904e-05, 2.5815628062284942e-05, 2.5927010245248413e-05, 1.882002333771017e-05, 5.235061218203904e-05, 2.5815628062284942e-05, 3.074004661625544e-05, 2.5815628062284942e-05, 1.7963817027134377e-05, 2.362491267467673e-05, 1.9597097001739e-05, 2.6881548349820885e-05, 3.074004661625544e-05, 1.9597097001739e-05, 2.2410006579131825e-05, 3.110031123877285e-05, 2.6881548349820885e-05, 2.5815628062284942e-05, 3.074004661625544e-05, ... ]\n",
      "[1.9605331743486973e-05, 1.7971365461170967e-05, 2.2419423311453615e-05, 2.689284403348397e-05, 3.075296364907836e-05, 3.075296364907836e-05, 3.075296364907836e-05, 2.689284403348397e-05, 3.557930252211e-05, 2.689284403348397e-05, 1.7230192411927546e-05, 1.6273919204274127e-05, 2.2323109687723337e-05, 1.882793155145546e-05, 3.075296364907836e-05, 2.2323109687723337e-05, 3.075296364907836e-05, 1.7230192411927546e-05, 3.111337965555469e-05, 4.526825592387652e-05, 3.075296364907836e-05, 1.9605331743486973e-05, 3.111337965555469e-05, 4.526825592387652e-05, 2.689284403348397e-05, 2.2323109687723337e-05, 2.2323109687723337e-05, 2.2323109687723337e-05, 3.075296364907836e-05, 3.557930252211e-05, 2.689284403348397e-05, 3.557930252211e-05, 1.882793155145546e-05, 2.593790482997429e-05, 3.075296364907836e-05, 3.075296364907836e-05, 3.075296364907836e-05, 4.526825592387652e-05, 1.9605331743486973e-05, 2.2419423311453615e-05, 4.526825592387652e-05, 2.2419423311453615e-05, 1.6273919204274127e-05, 1.7230192411927546e-05, 1.9605331743486973e-05, 3.075296364907836e-05, 3.075296364907836e-05, 3.075296364907836e-05, 5.237261002037342e-05, 3.075296364907836e-05, 3.075296364907836e-05, 4.526825592387652e-05, 3.557930252211e-05, 2.2419423311453615e-05, 3.075296364907836e-05, 3.3001313424926466e-05, 2.363483991311807e-05, 1.9605331743486973e-05, 2.689284403348397e-05, 2.689284403348397e-05, 2.2419423311453615e-05, 2.2323109687723337e-05, 3.557930252211e-05, 2.2323109687723337e-05, 2.2323109687723337e-05, 4.526825592387652e-05, 2.689284403348397e-05, 3.3001313424926466e-05, 1.7230192411927546e-05, 1.9605331743486973e-05, 2.689284403348397e-05, 3.075296364907836e-05, 1.7230192411927546e-05, 5.237261002037342e-05, 2.689284403348397e-05, 1.7230192411927546e-05, 2.2323109687723337e-05, 1.6273919204274127e-05, 3.075296364907836e-05, 2.689284403348397e-05, 5.237261002037342e-05, 5.237261002037342e-05, 2.5826475843981168e-05, 2.593790482997429e-05, 1.882793155145546e-05, 5.237261002037342e-05, 2.5826475843981168e-05, 3.075296364907836e-05, 2.5826475843981168e-05, 1.7971365461170967e-05, 2.363483991311807e-05, 1.9605331743486973e-05, 2.689284403348397e-05, 3.075296364907836e-05, 1.9605331743486973e-05, 2.2419423311453615e-05, 3.111337965555469e-05, 2.689284403348397e-05, 2.5826475843981168e-05, 3.075296364907836e-05, ... ]\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "is_correct\n",
      "[0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, ... ]\n",
      "19075\n",
      "is_wrong\n",
      "[1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, ... ]\n",
      "18149\n",
      "weighted_error\n",
      "0.487021661048\n",
      "weight\n",
      "0.0259625096915\n",
      "adjustment\n",
      "[1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 0.9743716184165552, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 0.9743716184165552, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 0.9743716184165552, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 0.9743716184165552, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, 1.0263024713559425, ... ]\n",
      "[2.012100042009379e-05, 1.8444056786440593e-05, 2.3009109550919876e-05, 2.760019229335451e-05, 3.1561842594568585e-05, 3.1561842594568585e-05, 3.1561842594568585e-05, 2.760019229335451e-05, 3.651512610756222e-05, 2.760019229335451e-05, 1.768338905429965e-05, 1.670196349799347e-05, 2.2910262640860245e-05, 1.9323152681779262e-05, 3.1561842594568585e-05, 2.2910262640860245e-05, 3.1561842594568585e-05, 1.768338905429965e-05, 3.193173843273148e-05, 4.645892292864776e-05, 3.1561842594568585e-05, 2.012100042009379e-05, 3.193173843273148e-05, 4.410810378744238e-05, 2.760019229335451e-05, 2.2910262640860245e-05, 2.2910262640860245e-05, 2.2910262640860245e-05, 3.1561842594568585e-05, 3.651512610756222e-05, 2.760019229335451e-05, 3.651512610756222e-05, 1.9323152681779262e-05, 2.662013582879785e-05, 3.1561842594568585e-05, 3.1561842594568585e-05, 3.1561842594568585e-05, 4.645892292864776e-05, 2.012100042009379e-05, 2.3009109550919876e-05, 4.645892292864776e-05, 2.3009109550919876e-05, 1.670196349799347e-05, 1.768338905429965e-05, 2.012100042009379e-05, 3.1561842594568585e-05, 3.1561842594568585e-05, 3.1561842594568585e-05, 5.375013909527024e-05, 3.1561842594568585e-05, 3.1561842594568585e-05, 4.645892292864776e-05, 3.651512610756222e-05, 2.3009109550919876e-05, 3.1561842594568585e-05, 3.3869329525994074e-05, 2.4256494612935146e-05, 2.012100042009379e-05, 2.6203623964729775e-05, 2.760019229335451e-05, 2.3009109550919876e-05, 2.2910262640860245e-05, 3.651512610756222e-05, 2.2910262640860245e-05, 2.2910262640860245e-05, 4.645892292864776e-05, 2.760019229335451e-05, 3.3869329525994074e-05, 1.768338905429965e-05, 2.012100042009379e-05, 2.760019229335451e-05, 3.1561842594568585e-05, 1.768338905429965e-05, 5.1030384786250346e-05, 2.760019229335451e-05, 1.768338905429965e-05, 2.2910262640860245e-05, 1.670196349799347e-05, 3.1561842594568585e-05, 2.760019229335451e-05, 5.375013909527024e-05, 5.375013909527024e-05, 2.5164585066096e-05, 2.662013582879785e-05, 1.9323152681779262e-05, 5.375013909527024e-05, 2.6505775985092424e-05, 3.1561842594568585e-05, 2.6505775985092424e-05, 1.8444056786440593e-05, 2.4256494612935146e-05, 2.012100042009379e-05, 2.760019229335451e-05, 3.1561842594568585e-05, 2.012100042009379e-05, 2.3009109550919876e-05, 3.193173843273148e-05, 2.760019229335451e-05, 2.6505775985092424e-05, 3.1561842594568585e-05, ... ]\n",
      "[2.012778210039538e-05, 1.8450273261465618e-05, 2.3016864653635486e-05, 2.760949479703167e-05, 3.157248035222162e-05, 3.157248035222162e-05, 3.157248035222162e-05, 2.760949479703167e-05, 3.652743334409438e-05, 2.760949479703167e-05, 1.7689349150155283e-05, 1.670759280938393e-05, 2.291798442773028e-05, 1.93296654515805e-05, 3.157248035222162e-05, 2.291798442773028e-05, 3.157248035222162e-05, 1.7689349150155283e-05, 3.1942500861885285e-05, 4.647458167105024e-05, 3.157248035222162e-05, 2.012778210039538e-05, 3.1942500861885285e-05, 4.4122970198274376e-05, 2.760949479703167e-05, 2.291798442773028e-05, 2.291798442773028e-05, 2.291798442773028e-05, 3.157248035222162e-05, 3.652743334409438e-05, 2.760949479703167e-05, 3.652743334409438e-05, 1.93296654515805e-05, 2.662910800945521e-05, 3.157248035222162e-05, 3.157248035222162e-05, 3.157248035222162e-05, 4.647458167105024e-05, 2.012778210039538e-05, 2.3016864653635486e-05, 4.647458167105024e-05, 2.3016864653635486e-05, 1.670759280938393e-05, 1.7689349150155283e-05, 2.012778210039538e-05, 3.157248035222162e-05, 3.157248035222162e-05, 3.157248035222162e-05, 5.376825530479759e-05, 3.157248035222162e-05, 3.157248035222162e-05, 4.647458167105024e-05, 3.652743334409438e-05, 2.3016864653635486e-05, 3.157248035222162e-05, 3.388074501031787e-05, 2.426467014040733e-05, 2.012778210039538e-05, 2.621245576219321e-05, 2.760949479703167e-05, 2.3016864653635486e-05, 2.291798442773028e-05, 3.652743334409438e-05, 2.291798442773028e-05, 2.291798442773028e-05, 4.647458167105024e-05, 2.760949479703167e-05, 3.388074501031787e-05, 1.7689349150155283e-05, 2.012778210039538e-05, 2.760949479703167e-05, 3.157248035222162e-05, 1.7689349150155283e-05, 5.104758431649547e-05, 2.760949479703167e-05, 1.7689349150155283e-05, 2.291798442773028e-05, 1.670759280938393e-05, 3.157248035222162e-05, 2.760949479703167e-05, 5.376825530479759e-05, 5.376825530479759e-05, 2.517306666081184e-05, 2.662910800945521e-05, 1.93296654515805e-05, 5.376825530479759e-05, 2.6514709621349248e-05, 3.157248035222162e-05, 2.6514709621349248e-05, 1.8450273261465618e-05, 2.426467014040733e-05, 2.012778210039538e-05, 2.760949479703167e-05, 3.157248035222162e-05, 2.012778210039538e-05, 2.3016864653635486e-05, 3.1942500861885285e-05, 2.760949479703167e-05, 2.6514709621349248e-05, 3.157248035222162e-05, ... ]\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, \n",
    "                                target, num_tree_stumps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Recall from the lecture that in order to make predictions, we use the following formula:\n",
    "$$\n",
    "\\hat{y} = sign\\left(\\sum_{t=1}^T \\hat{w}_t f_t(x)\\right)\n",
    "$$\n",
    "\n",
    "We need to do the following things:\n",
    "- Compute the predictions $f_t(x)$ using the $t$-th decision tree\n",
    "- Compute $\\hat{w}_t f_t(x)$ by multiplying the `stump_weights` with the predictions $f_t(x)$ from the decision trees\n",
    "- Sum the weighted predictions over each stump in the ensemble.\n",
    "\n",
    "Complete the following skeleton for making predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_adaboost(stump_weights, tree_stumps, data):\n",
    "    scores = graphlab.SArray([0.]*len(data))\n",
    "    \n",
    "    for i, tree_stump in enumerate(tree_stumps):\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x))\n",
    "        \n",
    "        # Accumulate predictions on scores array\n",
    "        # YOUR CODE HERE\n",
    "        scores=scores+predictions*stump_weights[i]\n",
    "        \n",
    "    return scores.apply(lambda score : +1 if score > 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 10-component ensemble = 0.620314519604\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_adaboost(stump_weights, tree_stumps, test_data)\n",
    "accuracy = graphlab.evaluation.accuracy(test_data[target], predictions)\n",
    "print 'Accuracy of 10-component ensemble = %s' % accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us take a quick look what the `stump_weights` look like at the end of each iteration of the 10-stump ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15802933659263743,\n",
       " 0.1768236329364191,\n",
       " 0.09311888971129693,\n",
       " 0.07288885525840554,\n",
       " 0.06706306914118143,\n",
       " 0.06456916961644447,\n",
       " 0.05456055779178564,\n",
       " 0.04351093673362621,\n",
       " 0.02898871150041245,\n",
       " 0.02596250969152032]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stump_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** Are the weights monotonically decreasing, monotonically increasing, or neither?\n",
    "\n",
    "**Reminder**: Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance plots\n",
    "\n",
    "In this section, we will try to reproduce some of the performance plots dicussed in the lecture.\n",
    "\n",
    "### How does accuracy change with adding stumps to the ensemble?\n",
    "\n",
    "We will now train an ensemble with:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 30`\n",
    "\n",
    "Once we are done with this, we will then do the following:\n",
    "* Compute the classification error at the end of each iteration.\n",
    "* Plot a curve of classification error vs iteration.\n",
    "\n",
    "First, lets train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership.MORTGAGE. (19846, 17378)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (19846 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (17378 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 10\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 11\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 12\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 13\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.4 years. (34593, 2631)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34593 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2631 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 14\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 15\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.C. (27812, 9412)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (27812 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9412 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 16\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 17\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 18\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 19\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 20\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 21\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 22\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 23\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 24\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 25\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.2 years. (33652, 3572)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33652 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3572 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 26\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 27\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership.OWN. (34149, 3075)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34149 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3075 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 28\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 29\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade.C. (27812, 9412)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (27812 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9412 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "# this may take a while... \n",
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, \n",
    "                                 features, target, num_tree_stumps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing training error at the end of each iteration\n",
    "\n",
    "Now, we will compute the classification error on the **train_data** and see how it is reduced as trees are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training error = 0.421636578551\n",
      "Iteration 2, training error = 0.433430045132\n",
      "Iteration 3, training error = 0.400037610144\n",
      "Iteration 4, training error = 0.400037610144\n",
      "Iteration 5, training error = 0.384724908661\n",
      "Iteration 6, training error = 0.384617451107\n",
      "Iteration 7, training error = 0.382763808296\n",
      "Iteration 8, training error = 0.384617451107\n",
      "Iteration 9, training error = 0.382763808296\n",
      "Iteration 10, training error = 0.384483129164\n",
      "Iteration 11, training error = 0.382736943907\n",
      "Iteration 12, training error = 0.381447453256\n",
      "Iteration 13, training error = 0.381528046422\n",
      "Iteration 14, training error = 0.380560928433\n",
      "Iteration 15, training error = 0.380507199656\n",
      "Iteration 16, training error = 0.378223726628\n",
      "Iteration 17, training error = 0.378277455405\n",
      "Iteration 18, training error = 0.378411777348\n",
      "Iteration 19, training error = 0.378062540297\n",
      "Iteration 20, training error = 0.378761014399\n",
      "Iteration 21, training error = 0.379566946056\n",
      "Iteration 22, training error = 0.378895336342\n",
      "Iteration 23, training error = 0.378895336342\n",
      "Iteration 24, training error = 0.378761014399\n",
      "Iteration 25, training error = 0.378895336342\n",
      "Iteration 26, training error = 0.378975929508\n",
      "Iteration 27, training error = 0.379110251451\n",
      "Iteration 28, training error = 0.378922200731\n",
      "Iteration 29, training error = 0.379029658285\n",
      "Iteration 30, training error = 0.378734150011\n"
     ]
    }
   ],
   "source": [
    "error_all = []\n",
    "for n in xrange(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n",
    "    error = 1.0 - graphlab.evaluation.accuracy(train_data[target], predictions)\n",
    "    error_all.append(error)\n",
    "    print \"Iteration %s, training error = %s\" % (n, error_all[n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training error vs number of iterations\n",
    "\n",
    "We have provided you with a simple code snippet that plots classification error with the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdoAAAFgCAYAAAActbi8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18704b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Which of the following best describes a **general trend in accuracy** as we add more and more components? Answer based on the 30 components learned so far.\n",
    "\n",
    "1. Training error goes down monotonically, i.e. the training error reduces with each iteration but never increases.\n",
    "2. Training error goes down in general, with some ups and downs in the middle.\n",
    "3. Training error goes up in general, with some ups and downs in the middle.\n",
    "4. Training error goes down in the beginning, achieves the best error, and then goes up sharply.\n",
    "5. None of the above\n",
    "\n",
    "\n",
    "### Evaluation on the test data\n",
    "\n",
    "Performing well on the training data is cheating, so lets make sure it works on the `test_data` as well. Here, we will compute the classification error on the `test_data` at the end of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, test error = 0.42330891857\n",
      "Iteration 2, test error = 0.428479103835\n",
      "Iteration 3, test error = 0.398104265403\n",
      "Iteration 4, test error = 0.398104265403\n",
      "Iteration 5, test error = 0.379900904782\n",
      "Iteration 6, test error = 0.380008616975\n",
      "Iteration 7, test error = 0.379254631624\n",
      "Iteration 8, test error = 0.380008616975\n",
      "Iteration 9, test error = 0.379254631624\n",
      "Iteration 10, test error = 0.379685480396\n",
      "Iteration 11, test error = 0.379254631624\n",
      "Iteration 12, test error = 0.377962085308\n",
      "Iteration 13, test error = 0.379254631624\n",
      "Iteration 14, test error = 0.377854373115\n",
      "Iteration 15, test error = 0.378500646273\n",
      "Iteration 16, test error = 0.377854373115\n",
      "Iteration 17, test error = 0.377962085308\n",
      "Iteration 18, test error = 0.377854373115\n",
      "Iteration 19, test error = 0.378177509694\n",
      "Iteration 20, test error = 0.376884963378\n",
      "Iteration 21, test error = 0.377531236536\n",
      "Iteration 22, test error = 0.376777251185\n",
      "Iteration 23, test error = 0.376777251185\n",
      "Iteration 24, test error = 0.376884963378\n",
      "Iteration 25, test error = 0.376777251185\n",
      "Iteration 26, test error = 0.376561826799\n",
      "Iteration 27, test error = 0.376454114606\n",
      "Iteration 28, test error = 0.376992675571\n",
      "Iteration 29, test error = 0.376777251185\n",
      "Iteration 30, test error = 0.376777251185\n"
     ]
    }
   ],
   "source": [
    "test_error_all = []\n",
    "for n in xrange(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], test_data)\n",
    "    error = 1.0 - graphlab.evaluation.accuracy(test_data[target], predictions)\n",
    "    test_error_all.append(error)\n",
    "    print \"Iteration %s, test error = %s\" % (n, test_error_all[n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize both the training and test errors\n",
    "\n",
    "Now, let us plot the training & test error with the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeIAAAFSCAYAAAAuI9zWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsQQ2swAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b9cb390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.plot(range(1,31), test_error_all, '-', linewidth=4.0, label='Test error')\n",
    "\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Quiz Question:** From this plot (with 30 trees), is there massive overfitting as the # of iterations increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
